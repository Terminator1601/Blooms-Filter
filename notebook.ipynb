{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dummy data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10000000 dummy records in dummy_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import csv\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize the Faker library\n",
    "fake = Faker()\n",
    "\n",
    "# Function to generate a SHA-256 hash of a given string\n",
    "def generate_hash(input_string):\n",
    "    return hashlib.sha256(input_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Function to create a CSV file with dummy hashes\n",
    "def generate_dummy_data(file_path, num_records):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['hash1', 'hash2', 'hash3']  # Define column headers\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write header\n",
    "        csvwriter.writeheader()\n",
    "        \n",
    "        for _ in range(num_records):\n",
    "            # Generate three fake file names\n",
    "            fake_file_name1 = fake.file_name()\n",
    "            fake_file_name2 = fake.file_name()\n",
    "            fake_file_name3 = fake.file_name()\n",
    "            \n",
    "            # Generate hashes for the fake file names\n",
    "            file_hash1 = generate_hash(fake_file_name1)\n",
    "            file_hash2 = generate_hash(fake_file_name2)\n",
    "            file_hash3 = generate_hash(fake_file_name3)\n",
    "            \n",
    "            # Write the hashes to the CSV file\n",
    "            csvwriter.writerow({'hash1': file_hash1, 'hash2': file_hash2, 'hash3': file_hash3})\n",
    "\n",
    "# Generate dummy data and write to a CSV file\n",
    "dummy_data_file_path = 'dummy_data.csv'  # Path to the output CSV file\n",
    "num_dummy_records = 10000000  # Number of dummy records to generate\n",
    "\n",
    "generate_dummy_data(dummy_data_file_path, num_dummy_records)\n",
    "print(f\"Generated {num_dummy_records} dummy records in {dummy_data_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating n numbers of columns and 100000 numbers of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import hashlib\n",
    "import csv\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize the Faker library\n",
    "fake = Faker()\n",
    "\n",
    "# Function to generate a SHA-256 hash of a given string\n",
    "def generate_hash(input_string):\n",
    "    return hashlib.sha256(input_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Function to create a CSV file with dummy hashes and a variable number of columns\n",
    "def generate_dummy_data(file_path, num_records, num_columns):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        # Define column headers dynamically based on the number of columns\n",
    "        fieldnames = [f'hash{i+1}' for i in range(num_columns)]\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write header\n",
    "        csvwriter.writeheader()\n",
    "        \n",
    "        for _ in range(num_records):\n",
    "            row = {}\n",
    "            for i in range(num_columns):\n",
    "                # Generate a fake file name and its hash\n",
    "                fake_file_name = fake.file_name()\n",
    "                file_hash = generate_hash(fake_file_name)\n",
    "                \n",
    "                # Add the hash to the row dictionary\n",
    "                row[f'hash{i+1}'] = file_hash\n",
    "            \n",
    "            # Write the row to the CSV file\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "# Generate dummy data and write to a CSV file\n",
    "dummy_data_file_path = 'dummy_data.csv'  # Path to the output CSV file\n",
    "num_dummy_records = 10000000  # Number of dummy records to generate\n",
    "num_columns = 5  # Number of columns to generate\n",
    "\n",
    "generate_dummy_data(dummy_data_file_path, num_dummy_records, num_columns)\n",
    "print(f\"Generated {num_dummy_records} dummy records with {num_columns} columns in {dummy_data_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from bloom_filter2 import BloomFilter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load NSRL database hashes from CSV into the Bloom filter\n",
    "def load_nsrl_into_bloom_filter(file_path, bloom_filter):\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # Skip the header row\n",
    "        for row in csvreader:\n",
    "            file_hash = row[0].strip().lower()\n",
    "            bloom_filter.add(file_hash)\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to load data into Bloom filter: {end_time - start_time} seconds\")\n",
    "\n",
    "# Function to check if a hash is in the Bloom filter\n",
    "def check_hash_in_bloom_filter(file_hash, bloom_filter):\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    hashed_value = file_hash.strip().lower()\n",
    "    result = hashed_value in bloom_filter\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to check hash: {end_time - start_time} seconds\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data into Bloom filter\n",
    "def load_data():\n",
    "    bloom_filter = BloomFilter(max_elements=1000000, error_rate=0.001)\n",
    "    dummy_data_file_path = 'dummy_nsrl_data.csv'  # Path to the generated CSV file\n",
    "    load_nsrl_into_bloom_filter(dummy_data_file_path, bloom_filter)\n",
    "    return bloom_filter\n",
    "\n",
    "# Function to test hashes against the Bloom filter\n",
    "def test_hashes(bloom_filter):\n",
    "    test_hash = '71249f9ec32e95e2be830139bcefd2822ad7d2f8d0554311c8aa32c3f1eb60ba'  # Example SHA-256 hash\n",
    "    if check_hash_in_bloom_filter(test_hash, bloom_filter):\n",
    "        print(\"Hash is in the NSRL database.\")\n",
    "    else:\n",
    "        print(\"Hash is NOT in the NSRL database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data into Bloom filter: 379.50721621513367 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load data into Bloom filter\n",
    "bloom_filter = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to check hash: 0.0 seconds\n",
      "Hash is in the NSRL database.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test hashes against the Bloom filter\n",
    "test_hashes(bloom_filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Column Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from bloom_filter2 import BloomFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load NSRL database hashes from CSV into the Bloom filter\n",
    "def load_nsrl_into_bloom_filter(file_path, bloom_filter):\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # Skip the header row\n",
    "        for row in csvreader:\n",
    "            # Assuming the hash is in the first column (index 0)\n",
    "            file_hash = row[0].strip().lower()\n",
    "            bloom_filter.add(file_hash)\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to load data into Bloom filter: {end_time - start_time} seconds\")\n",
    "\n",
    "# Function to check if a hash is in the Bloom filter\n",
    "def check_hash_in_bloom_filter(file_hash, bloom_filter):\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    hashed_value = file_hash.strip().lower()\n",
    "    result = hashed_value in bloom_filter\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to check hash: {end_time - start_time} seconds\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data into Bloom filter\n",
    "def load_data():\n",
    "    bloom_filter = BloomFilter(max_elements=10000000, error_rate=0.001)\n",
    "    dummy_data_file_path = 'dummy_data.csv'  # Path to the generated CSV file\n",
    "    load_nsrl_into_bloom_filter(dummy_data_file_path, bloom_filter)\n",
    "    return bloom_filter\n",
    "\n",
    "# Function to test hashes against the Bloom filter\n",
    "def test_hashes(bloom_filter):\n",
    "    test_hash = 'be5e61687477e9b7189649fac42bcbbaa62870b50c55b41817cf78b53bea5435'  # Example SHA-256 hash\n",
    "    if check_hash_in_bloom_filter(test_hash, bloom_filter):\n",
    "        print(\"Hash is in the NSRL database.\")\n",
    "    else:\n",
    "        print(\"Hash is NOT in the NSRL database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data into Bloom filter: 395.66683650016785 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load data into Bloom filter\n",
    "bloom_filter = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to check hash: 0.0 seconds\n",
      "Hash is in the NSRL database.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test hashes against the Bloom filter\n",
    "test_hashes(bloom_filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterative way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from bloom_filter2 import BloomFilter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load NSRL database hashes from CSV into the Bloom filter\n",
    "def load_nsrl_into_bloom_filter(file_path, bloom_filter):\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # Skip the header row\n",
    "        for row in csvreader:\n",
    "            # Assuming the hash is in the first column (index 0)\n",
    "            file_hash = row[0].strip().lower()\n",
    "            bloom_filter.add(file_hash)\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to load data into Bloom filter: {end_time - start_time} seconds\")\n",
    "\n",
    "# Function to check if a single hash is in the Bloom filter\n",
    "def check_hash_in_bloom_filter(file_hash, bloom_filter):\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    hashed_value = file_hash.strip().lower()\n",
    "    result = hashed_value in bloom_filter\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to check hash: {end_time - start_time} seconds\")\n",
    "    return result\n",
    "\n",
    "# Function to check multiple hashes iteratively against the Bloom filter\n",
    "def check_hashes_iterative(hash_list, bloom_filter):\n",
    "    results = []\n",
    "    start_time = time.time()  # Start the stopwatch\n",
    "    for file_hash in hash_list:\n",
    "        hashed_value = file_hash.strip().lower()\n",
    "        results.append(hashed_value in bloom_filter)\n",
    "    end_time = time.time()  # Stop the stopwatch\n",
    "    print(f\"Time taken to check {len(hash_list)} hashes iteratively: {end_time - start_time} seconds\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data into Bloom filter\n",
    "def load_data():\n",
    "    bloom_filter = BloomFilter(max_elements=10000000, error_rate=0.001)\n",
    "    dummy_data_file_path = 'dummy_data.csv'  # Path to the generated CSV file\n",
    "    load_nsrl_into_bloom_filter(dummy_data_file_path, bloom_filter)\n",
    "    return bloom_filter\n",
    "\n",
    "# Function to test hashes against the Bloom filter\n",
    "def test_hashes(bloom_filter):\n",
    "    test_hash = '53e30484d69f5f0afe07ac294d87bdb7b8f4c60daaf200309057740b7c56b508'  # Example SHA-256 hash\n",
    "    if check_hash_in_bloom_filter(test_hash, bloom_filter):\n",
    "        print(\"Hash is in the NSRL database.\")\n",
    "    else:\n",
    "        print(\"Hash is NOT in the NSRL database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load data into Bloom filter\n",
    "bloom_filter = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing bloom filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to check hash: 0.0 seconds\n",
      "Hash is in the NSRL database.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test a single hash against the Bloom filter\n",
    "test_hashes(bloom_filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to check 2 hashes iteratively: 0.0005152225494384766 seconds\n",
      "Hash e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 is NOT in the NSRL database.\n",
      "Hash 53e30484d69f5f0afe07ac294d87bdb7b8f4c60daaf200309057740b7c56b508 is in the NSRL database.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test multiple hashes iteratively against the Bloom filter\n",
    "hash_list = [\n",
    "    'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855',\n",
    "    '53e30484d69f5f0afe07ac294d87bdb7b8f4c60daaf200309057740b7c56b508'\n",
    "]\n",
    "results = check_hashes_iterative(hash_list, bloom_filter)\n",
    "for i, result in enumerate(results):\n",
    "    if result:\n",
    "        print(f\"Hash {hash_list[i]} is in the NSRL database.\")\n",
    "    else:\n",
    "        print(f\"Hash {hash_list[i]} is NOT in the NSRL database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
